{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b18a657-b1e6-481d-bdc2-04d8f1ef8c1b",
   "metadata": {},
   "source": [
    "### Student Information \n",
    "Name: 林靖淵<br>\n",
    "Student ID: 113356040<br>\n",
    "GitHub ID: https://github.com/jing-yuan-nccu<br>\n",
    "Kaggle name: jingyaun_nccu<br>\n",
    "Kaggle private scoreboard snapshot: <br>\n",
    "***\n",
    "### Instructions\n",
    "1. First: This part is worth 30% of your grade. Do the take home exercises in the DM2024-Lab2-master Repo. You may need to copy some cells from the Lab notebook to this notebook.\n",
    "2. Second: This part is worth 30% of your grade. Participate in the in-class Kaggle Competition regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place\n",
    "in the Private Leaderboard ranking:\n",
    "Bottom 40%: Get 20% of the 30% available for this section.\n",
    "Top 41% - 100%: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)\n",
    "Submit your last submission BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday). Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the img folder of this repository and rerun the cell Student Information.\n",
    "3. Third: This part is worth 30% of your grade. A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model.\n",
    "You can also mention different things you tried and insights you gained.\n",
    "4. Fourth: This part is worth 10% of your grade. It's hard for us to follow if your code is messy :'(, so please tidy up your notebook.\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "Make sure to commit and save your changes to your repository BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58fa36e-6b70-4021-a621-74953e0f10d4",
   "metadata": {},
   "source": [
    "## Third"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468f983-0396-45af-b6b1-24bec93a6123",
   "metadata": {},
   "source": [
    "In the Kaggle competition, I tried several methods. For embedding, I primarily used TF-IDF, though I also attempted using Word2Vec, but the process took too much time. Regarding model selection, I experimented with three machine learning models: Random Forest, Decision Tree, and XGBoost. Additionally, I also tried using deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc87ec9-782d-4db7-803d-bf7bb8a29a87",
   "metadata": {},
   "source": [
    "**Import module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3409a8f-b928-4ae2-a5af-8f6cddb315f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed5a2f-f56d-4afe-b418-8fd696f6db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e7f98-78fe-410e-966f-e0ee296b524c",
   "metadata": {},
   "source": [
    "***Data login & processing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4530c-b3b2-49d8-9704-0c2048b4a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    " \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e4c13-ce96-4ba6-a352-d5f048af3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = pd.read_csv('dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "data_identification = pd.read_csv('dm-2024-isa-5810-lab-2-homework/data_identification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b58ecb-e8a0-467a-a237-413e08114f0c",
   "metadata": {},
   "source": [
    "**1. Data Clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9bc65-4565-45f3-9ad7-e4fcc58fd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "def clean_text(text):\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e992a-b110-42fe-bb27-493f543369cc",
   "metadata": {},
   "source": [
    "**2. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738686b-cec3-4c75-96ed-218bf24145d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c168b6a-327d-4ea7-8137-b72f8eb7c7c2",
   "metadata": {},
   "source": [
    "**3. Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df660a-056e-4068-a8c7-4bdcdc399091",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "df[\"lemmatized_tokens\"] = df[\"tokens\"].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888d1c5-b3ee-4979-9e3f-9adac24f6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_source = df['_source'].apply(lambda x: x['tweet'])\n",
    "df = pd.DataFrame({\n",
    "    'tweet_id': _source.apply(lambda x: x['tweet_id']),\n",
    "    'hashtags': _source.apply(lambda x: x['hashtags']),\n",
    "    'text': _source.apply(lambda x: x['text']),\n",
    "})\n",
    "df = df.merge(data_identification, on='tweet_id', how='left')\n",
    "\n",
    "train_data = df[df['identification'] == 'train']\n",
    "test_data = df[df['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd56a9-5e9e-4648-9769-177e0eb9d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(emotion, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97ef74-3eac-460c-a57a-ab7502463352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(subset=['text'], keep=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2b6b3-a92b-4b94-9af8-9b4e0324b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d0b9e-cf9c-4feb-9bc8-047836c14947",
   "metadata": {},
   "source": [
    "***Prepare data for training model***<br>\n",
    "Due to the large size of the dataset, I chose to sample a certain percentage of the data for training. Even so, using just 20% of the data still required about an hour of training. Once I found suitable parameter combinations, I used the entire dataset for training. While this approach is not entirely rigorous, it serves as a useful reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ac35a-cede-4309-8a4c-5585c09a66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271b472-7042-4ec1-8726-95e985031e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data = train_data['emotion']\n",
    "X_train_data = train_data.drop(['tweet_id', 'emotion', 'identification', 'hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6cda08-8118-49dd-8d71-13eedddc357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_data, y_train_data, test_size=0.2, random_state=42, stratify=y_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666002d7-b864-4583-81c7-3ecdef51170a",
   "metadata": {},
   "source": [
    "**Feature engineering : TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32febd8-dc66-464a-88a1-3b6d47a0e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "X = tfidf.fit_transform(X_train['text']).toarray()\n",
    "X_test = tfidf.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fcb69-bd87-4718-af18-61ed46c726d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90dea69-df6f-4986-af7b-12ec4f988b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(le.classes_, range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1269399-68f0-4676-a39e-b70554549cf3",
   "metadata": {},
   "source": [
    "***Training model & Accuracy***<br>\n",
    "For the machine learning model selection, I tried XGBoost, Random Forest, and Decision Tree. Among them, XGBoost performed very poorly, producing outputs limited to only three emotion categories, resulting in poor outcomes. Initially, I determined that this was not a data issue, as the input was consistent across all models. Therefore, I decided not to use XGBoost as my model. As for Decision Tree and Random Forest, while Decision Tree trained faster, Random Forest produced better results. Ultimately, I chose to fine-tune Random Forest to find the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40fa92-f3bd-4968-829a-65e3e6cea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "clf = XGBClassifier()\n",
    "# Fit the classifier to your data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55320d1c-4073-4edb-ae3f-10904b471e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acdced-0b9f-49da-92b4-f74b795b55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "param_grid = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 20,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 3\n",
    "}\n",
    "\n",
    "# Train the RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13a7a9-68c0-40a1-bde3-f1f9b11bb179",
   "metadata": {},
   "source": [
    "***Try different combination of parameters***<br>\n",
    "I tried a total of five parameters for Random Forest, each with three different values, resulting in 3^5 combinations. Ultimately, I found that the combination of max_depth=20 and max_features=\"sqrt\" performed the best. However, the accuracy was around 0.44 for all combinations. The primary goal was to find a suitable set of parameters to avoid overfitting caused by a lack of parameter control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c72063-27b0-40f8-9336-028bf7b2ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Wrap the grid search process with tqdm\n",
    "def tqdm_grid_search(cv, estimator, param_grid, X, y, scoring='accuracy'):\n",
    "    param_list = list(ParameterGrid(param_grid))  # Generate all parameter combinations\n",
    "    results = []\n",
    "    for params in tqdm(param_list, desc=\"Grid Search Progress\"):\n",
    "        clf = estimator.set_params(**params)\n",
    "        scores = []\n",
    "        for train_idx, test_idx in cv.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            clf.fit(X_train, y_train)\n",
    "            scores.append(clf.score(X_test, y_test))\n",
    "        results.append((params, scores))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# Call tqdm_grid_search\n",
    "results = tqdm_grid_search(\n",
    "    cv=cv,\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    X=np.array(X),  # Convert to numpy array if needed\n",
    "    y=np.array(y),  # Convert to numpy array if needed\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Best Parameters and Scores:\")\n",
    "for params, scores in results:\n",
    "    print(f\"Params: {params}, Mean Accuracy: {np.mean(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cce27-dedf-4563-911e-6b39c144d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e168174-e16d-4c51-9d6f-6931ac421241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(accuracy_score(y, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85540eb-3eb2-4ea9-80da-140668fe5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred, target_names=le.classes_, digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e30b767-52f2-48a0-ae0e-ab4e7ee141eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55becd1d-9773-4dce-b54e-c222c5ee6fb2",
   "metadata": {},
   "source": [
    "### Deep learning \n",
    "I also tried using deep learning methods to train the model. Although the results were better than Random Forest, the training time was significantly longer. Therefore, I decided to first focus on fine-tuning the Random Forest model before adjusting the deep learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ea627-b60f-4eb9-b8c0-5c4797191e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4d076-d276-460a-a61e-488e0d1682ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(le.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d0802-6b8b-4d60-8281-9914d99306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96c805-9543-4051-b7c9-511d02dfc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "x = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(x)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e96d2-eef2-4b06-bbc5-66515ebc45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ee18f-286c-4638-8de1-d3308048d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X, y, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebeb96-d971-4236-9d51-42e57a99e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e289ec-6f0c-4cd7-af1d-ca8b08769dce",
   "metadata": {},
   "source": [
    "**Draw loss plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e24a8-a170-4c99-ac4f-c804fffa7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "df = pd.DataFrame(training_log)\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(df['epoch'], df['accuracy'], label='Accuracy', marker='o')\n",
    "plt.plot(df['epoch'], df['val_accuracy'], label='Validation Accuracy', marker='o')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Subplot 2: Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(df['epoch'], df['loss'], label='Loss', marker='o')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fab01d-15f7-499e-b32e-f61972d14ab3",
   "metadata": {},
   "source": [
    "### Use OpenAI embedding\n",
    "I originally planned to use OpenAI's API for embedding, but I found that it required a lot of time—processing 300K records would take over 3 hours. Therefore, I determined that this method was not suitable for my current situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb7bdc-c244-4eb9-9560-55a1de890770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data (replace with your actual data)\n",
    "X_train_texts = X_train['text'].tolist()  # Convert training text column to a list\n",
    "X_test_texts = X_test['text'].tolist()    # Convert testing text column to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5484ce-91d3-40b3-b9b7-050514e1bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tqdm import tqdm  # For showing progress bars\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set OpenAI API Key\n",
    "API_Key = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "# Function to generate embeddings from OpenAI\n",
    "def generate_embeddings(texts, model=\"text-embedding-ada-002\"):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        embedding = response.data[0].embedding\n",
    "        embeddings.append(embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Generate embeddings for train and test data\n",
    "train_embeddings = generate_embeddings(X_train_texts)\n",
    "test_embeddings = generate_embeddings(X_test_texts)\n",
    "\n",
    "# Reduce dimensions to 500 using Truncated SVD\n",
    "'''svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "train_embeddings_500 = svd.fit_transform(train_embeddings)\n",
    "test_embeddings_500 = svd.transform(test_embeddings)\n",
    "\n",
    "# Train embeddings are now reduced to 500 dimensions\n",
    "print(\"Shape of Train Embeddings:\", train_embeddings_500.shape)\n",
    "print(\"Shape of Test Embeddings:\", test_embeddings_500.shape)'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a9b82-2076-4a97-a369-6908aa0457db",
   "metadata": {},
   "source": [
    "### Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724549d-2f0d-48d1-a665-13ab393bace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = test_data.drop(['tweet_id', 'identification', 'hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e283f-9e06-4181-8528-8ce0ebaa1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = tfidf.transform(X_test_data['text']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d435b049-c2bf-4989-bfc1-25e04c057c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "y_test_pred = model.predict(X_test_data, batch_size=128)\n",
    "y_pred_labels = label_decode(label_encoder, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6fa0e-d901-4644-8bce-d2f3fcbfa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c1b14-32ff-4f60-b2d6-d5763c46f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "y_test_pred = clf.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151298d5-3b9c-4f9d-92e2-4c51cac184db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_labels = le.inverse_transform(y_test_pred)\n",
    "y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a3053-6645-425b-836f-274136b561ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test_data['tweet_id'],\n",
    "    'emotion': y_pred_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5f705-1c5a-48d6-977e-4d3cb6f7c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('kaggle/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
